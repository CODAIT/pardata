{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyDAX MVP Demo\n",
    "\n",
    "This notebook showcases early PyDAX functionality. \n",
    "\n",
    "  * GitHub repo: https://github.com/CODAIT/pydax\n",
    "  * Docs: https://pydax.readthedocs.io/en/latest/\n",
    "\n",
    "PyDAX is a Python API designed to simplify downloading and loading datasets using schemata. PyDAX was designed with the flexibility to allow users to use or specify their own schemata for the package to load metadata from. This means users can use PyDAX to load virtually any dataset regardless of data format or directory structure and in just a few simple lines of Python code. For example, if we wanted to load the [WikiText-103](https://developer.ibm.com/exchanges/data/all/wikitext-103/) dataset (included in the default PyDAX dataset schema), we would simply need to run:\n",
    "\n",
    "```python\n",
    ">>> import pydax\n",
    ">>> wikitext103_data = pydax.load_dataset('wikitext103')  # load the dataset (download if not yet downloaded)\n",
    ">>> print(wikitext103_data['train'][:500])  # preview the training split subdataset\n",
    "```\n",
    "```text\n",
    " = Valkyria Chronicles III = \n",
    " \n",
    " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs par\n",
    "```\n",
    "\n",
    "Out of the box, PyDAX comes with data loaders that support common data formats, while the dataset's directory structure is specified by the user in the dataset schema. The advantage of this setup is that it means when a user wants to share their dataset with the world, they have the option of bundling that dataset with a PyDAX dataset schema. Doing so allows anybody to securely download and load the dataset without needing to run any messy or potentially insecure scripts.\n",
    "\n",
    "## 1. Install & Load PyDAX\n",
    "\n",
    "The package has not yet been released on PyPI, so lets first install it from GitHub as well as import all necessary libraries. PyDAX is still in active development, so we'll pin the install to a specific commit hash for the purposes of this demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/CODAIT/pydax.git@e432fb03d1dbfe39fa3c3011cda7fb2e0c3090ef\n",
      "  Cloning git://github.com/CODAIT/pydax.git (to revision e432fb03d1dbfe39fa3c3011cda7fb2e0c3090ef) to /tmp/wsuser/pip-req-build-wp2_kbyh\n",
      "  Running command git clone -q git://github.com/CODAIT/pydax.git /tmp/wsuser/pip-req-build-wp2_kbyh\n",
      "  Running command git checkout -q e432fb03d1dbfe39fa3c3011cda7fb2e0c3090ef\n",
      "Requirement already satisfied: packaging>=20.4 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pydax==0.1.dev91+ge432fb0) (20.4)\n",
      "Collecting pandas>=1.1.0\n",
      "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic>=1.7.2\n",
      "  Downloading pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 23.1 MB/s eta 0:00:01     |███████████████████████████████ | 8.8 MB 23.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pydax==0.1.dev91+ge432fb0) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pydax==0.1.dev91+ge432fb0) (2.24.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from packaging>=20.4->pydax==0.1.dev91+ge432fb0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from packaging>=20.4->pydax==0.1.dev91+ge432fb0) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=1.1.0->pydax==0.1.dev91+ge432fb0) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=1.1.0->pydax==0.1.dev91+ge432fb0) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=1.1.0->pydax==0.1.dev91+ge432fb0) (1.18.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests>=2.24.0->pydax==0.1.dev91+ge432fb0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests>=2.24.0->pydax==0.1.dev91+ge432fb0) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests>=2.24.0->pydax==0.1.dev91+ge432fb0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests>=2.24.0->pydax==0.1.dev91+ge432fb0) (2.9)\n",
      "Building wheels for collected packages: pydax\n",
      "  Building wheel for pydax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pydax: filename=pydax-0.1.dev91+ge432fb0-py3-none-any.whl size=32229 sha256=6f27bd72fe92a6631da1154f4d16dd9629fff3e85477bc08c67624e8fc6af7c0\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/b6/49/47/63fa0755449ad2a8db32f8794cb6576db2151cc52d1d8b3c47\n",
      "Successfully built pydax\n",
      "\u001b[31mERROR: ibm-watson-machine-learning 1.0.44 has requirement pandas<=1.0.5, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas, pydantic, pydax\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "Successfully installed pandas-1.1.5 pydantic-1.7.3 pydax-0.1.dev91+ge432fb0\n"
     ]
    }
   ],
   "source": [
    "# Install PyDAX from GH from specific commit: https://github.com/CODAIT/pydax/tree/e432fb03d1dbfe39fa3c3011cda7fb2e0c3090ef\n",
    "!pip install git+git://github.com/CODAIT/pydax.git@e432fb03d1dbfe39fa3c3011cda7fb2e0c3090ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyDAX\n",
    "import pydax\n",
    "from pydax.dataset import Dataset\n",
    "from pydax.loaders import FormatLoaderMap, Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other packages\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.dev91+ge432fb0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the current version\n",
    "pydax.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset',\n",
       " 'export_schemata',\n",
       " 'get_config',\n",
       " 'get_dataset_metadata',\n",
       " 'init',\n",
       " 'list_all_datasets',\n",
       " 'load_dataset',\n",
       " 'load_schemata',\n",
       " 'loaders',\n",
       " 'schema']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at package makeup\n",
    "[symbol for symbol in dir(pydax) if not symbol.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Beginner Functionality\n",
    "\n",
    "Beginner users of PyDAX will use the package's high-level functions designed to provide core functionality (downloading and loading datasets) with minimal configurability. In this section we'll learn the easiest way to load and download datasets from a dataset schema by exploring the following high-level functions: `get_config`, `list_all_datasets`, `get_dataset_metadata`, and `load_dataset`.\n",
    "\n",
    "\n",
    "Use the `get_config` function to view the library's global configs. Currently the configs stored are:\n",
    "  * `DATADIR`: The default data directory used for downloading/loading datasets.\n",
    "  * `DATASET_SCHEMA_URL`: The URL for the default dataset schema to use. This schema is used to provide all necessary metadata for downloading/loading datasets (e.g. dataset download URL, dataset data format, dataset directory structure). We'll take a closer look closer at this schema later in the notebook.\n",
    "  * `FORMAT_SCHEMA_URL`: The URL for the default format schema to use. This schema is used to provide extra metadata regarding dataset formats.\n",
    "  * `LICENSE_SCHEMA_URL`: The URL for the default license schema to use. This schema is used to provide extra metadata regarding dataset licenses.\n",
    "  \n",
    "The default schemata are currently stored in: https://github.com/CODAIT/dax-schemata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyDAX default datadir:  /home/wsuser/.pydax/data\n",
      "PyDAX dataset schema URL:  https://raw.githubusercontent.com/CODAIT/dax-schemata/master/datasets.yaml\n",
      "PyDAX format schema URL:  https://raw.githubusercontent.com/CODAIT/dax-schemata/master/formats.yaml\n",
      "PyDAX license schema URL:  https://raw.githubusercontent.com/CODAIT/dax-schemata/master/licenses.yaml\n"
     ]
    }
   ],
   "source": [
    "# View default config settings\n",
    "print('PyDAX default datadir: ', pydax.get_config().DATADIR)\n",
    "print('PyDAX dataset schema URL: ', pydax.get_config().DATASET_SCHEMA_URL)\n",
    "print('PyDAX format schema URL: ', pydax.get_config().FORMAT_SCHEMA_URL)\n",
    "print('PyDAX license schema URL: ', pydax.get_config().LICENSE_SCHEMA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `list_all_datasets` function to view all available datasets and their versions as defined in the dataset schema loaded from `pydax.get_config().DATADIR`. The names and versions are allowable arguments to pass to the `name` and `version` parameters used by other PyDAX high-level functions such as `load_dataset()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim_sentences_search': ('1.0.2',),\n",
       " 'expert-in-the-loop-ai-polymer-discovery': ('1.0.0',),\n",
       " 'gmb': ('1.0.2',),\n",
       " 'wikitext103': ('1.0.1',),\n",
       " 'noaa_jfk': ('1.1.4',),\n",
       " 'taranaki-basin-curated-well-logs': ('1.0.0',)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all default datasets available to download using PyDAX (by default, these will be datasets from IBM's Data Asset Exchange: https://ibm.biz/data-exchange)\n",
    "pydax.list_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `get_dataset_metadata` function to peek at a dataset's metadata. Set the `human` parameter to `False` if you want to see the raw dictionary representation of the dataset's schema.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: WikiText-103\n",
      "Description: The WikiText-103 dataset is a collection of over 100 million tokens extracted from the set of verified ‘Good’ and ‘Featured’ articles on Wikipedia.\n",
      "Size: 181M\n",
      "Published date: 2020-03-17\n",
      "License: Creative Commons Attribution 3.0 International (CC BY 3.0)\n",
      "Available subdatasets: train, valid, test\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset WikiText-103's metadata in human-readable format\n",
    "print(pydax.get_dataset_metadata('wikitext103'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset into memory as a dict composed of subdatasets using `load_dataset`. By default, `load_dataset` will download the dataset to your default data directory if it is not already present there. If the `version` parameter is not provided, the dataset's latest version specified in the dataset schema is assumed. In this example, PyDAX will download the WikiText-103 dataset (version 1.0.1) to `~/.pydax/data/wikitext103/1.0.1/` and load its subdatasets into `wikitext103_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText-103, since it hasn't been downloaded yet, load_dataset will automatically download and unarchive the dataset, before loading it\n",
    "# WikiText-103 is 181 MB, so this cell may take a little bit of time to complete the first time it is run\n",
    "wikitext103_data = pydax.load_dataset('wikitext103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show available WikiText-103 subdatasets (subdatasets are PyDAX's way of modeling various dataset directory structures)\n",
    "wikitext103_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Homarus gammarus = \n",
      " \n",
      " Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# PyDAX loads plaintext datasets into strings by default\n",
    "# WikiText-103 is a plaintext dataset composed of Wikipedia articles, lets take a peek at its validation subdataset\n",
    "wikitext103_valid = wikitext103_data['valid']\n",
    "print(wikitext103_valid[:730])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515M\t/home/wsuser/.pydax/data/wikitext103/1.0.1/wikitext-103/wiki.train.tokens\r\n",
      "24K\t/home/wsuser/.pydax/data/wikitext103/1.0.1/wikitext-103/LICENSE.txt\r\n",
      "1.1M\t/home/wsuser/.pydax/data/wikitext103/1.0.1/wikitext-103/wiki.valid.tokens\r\n",
      "1.3M\t/home/wsuser/.pydax/data/wikitext103/1.0.1/wikitext-103/wiki.test.tokens\r\n",
      "4.0K\t/home/wsuser/.pydax/data/wikitext103/1.0.1/wikitext-103/README.txt\r\n",
      "517M\t/home/wsuser/.pydax/data/wikitext103/1.0.1/wikitext-103\r\n",
      "4.0K\t/home/wsuser/.pydax/data/wikitext103/1.0.1/.pydax.dataset/files.list\r\n",
      "8.0K\t/home/wsuser/.pydax/data/wikitext103/1.0.1/.pydax.dataset\r\n",
      "517M\t/home/wsuser/.pydax/data/wikitext103/1.0.1\r\n",
      "517M\t/home/wsuser/.pydax/data/wikitext103\r\n",
      "517M\t/home/wsuser/.pydax/data\r\n",
      "517M\t/home/wsuser/.pydax\r\n"
     ]
    }
   ],
   "source": [
    "# Show datadir structure\n",
    "!du -ah ~/.pydax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `load_dataset` downloads to and loads from `~/.pydax/data/<dataset-name>/<dataset-version>/`. To change the default data directory (and any other global configs), use `pydax.init`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/wsuser/new-pydax-datadir')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change default datadir to new-pydax-datadir\n",
    "new_pydax_datadir_path = pathlib.Path.home() / 'new-pydax-datadir'\n",
    "pydax.init(DATADIR=new_pydax_datadir_path)  # pass global configs to change to init() as kwargs\n",
    "pydax.get_config().DATADIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Functionality\n",
    "\n",
    "In this section, we cover PyDAX's low-level features prioritizing flexibility. We'll take a look at the low-level functionality of the package by working with the `Dataset`, `SchemaManager`, `Schema`, `Loaders`, and `FormatLoaderMap` classes.\n",
    "\n",
    "### 3.1 Dataset Class\n",
    "\n",
    "The main class of PyDAX is `pydax.dataset.Dataset`, which models a dataset. High-level functions use this class behind the scenes, however users who want access to more advanced features may want to interact with the class directly. \n",
    "\n",
    "`Dataset` requires a dataset `schema` and `data_dir` arguments to load/download a dataset. Let's first extract a *dataset* schema from our default *datasets* schema. To do this, we'll use the `export_schemata` function which is used to return copies of our datasets, licenses, and formats schemata as `Schema` objects. These copies are stored in a `SchemaManager` object as a dictionary accessible via the `schemata` attribute. To extract a certain chunk of a schema, you can call a `Schema` object's `export_schema` method, and supply it with the sequence of keys leading to the portion of the schema to be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'NOAA Weather Data – JFK Airport',\n",
       " 'published': datetime.date(2019, 9, 12),\n",
       " 'homepage': 'https://developer.ibm.com/exchanges/data/all/jfk-weather-data/',\n",
       " 'download_url': 'https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz',\n",
       " 'sha512sum': 'e3f27a8fcc0db5289df356e3f48aef6df56236798d5b3ae3889d358489ec6609d2d797e4c4932b86016d2ce4a379ac0a0749b6fb2c293ebae4e585ea1c8422ac',\n",
       " 'license': 'cdla_sharing',\n",
       " 'estimated_size': '3.2M',\n",
       " 'description': 'The NOAA JFK dataset contains 114,546 hourly observations of various local climatological variables (including visibility, temperature, wind speed and direction, humidity, dew point, and pressure). The data was collected by a NOAA weather station located at the John F. Kennedy International Airport in Queens, New York.',\n",
       " 'subdatasets': {'jfk_weather_cleaned': {'name': 'Cleaned JFK Weather Data',\n",
       "   'description': 'Cleaned version of the JFK weather data.',\n",
       "   'format': {'id': 'csv', 'options': {'columns': {'DATE': 'datetime'}}},\n",
       "   'path': 'noaa-weather-data-jfk-airport/jfk_weather_cleaned.csv'}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the default pydax schemata and extract the NOAA JFK Weather version 1.1.4 dataset schema from the datasets schema\n",
    "schema_manager = pydax.export_schemata()  # export copies of the datasets, licenses, and formats Schema objects into a SchemaManager object\n",
    "datasets_schema = schema_manager.schemata['datasets']  # extract the dataset Schema\n",
    "jfk_schema = datasets_schema.export_schema('datasets', 'noaa_jfk', '1.1.4')  # extract NOAA JFK Weather dataset schema version 1.1.4\n",
    "jfk_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` class also accepts an optional `mode` for how to instantiate the `Dataset`. Available modes include:\n",
    "- `LAZY` (default load mode: init's Dataset without download/loading)\n",
    "- `DOWNLOAD_ONLY`\n",
    "- `LOAD_ONLY`\n",
    "- `DOWNLOAD_AND_LOAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the NOAA JFK Weather dataset using the Dataset class in LAZY mode\n",
    "jfk_data_dir = pydax.get_config().DATADIR / 'jfk' / '1.1.4'\n",
    "jfk_dataset = Dataset(schema=jfk_schema, data_dir=jfk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Dataset.download() to download\n",
    "jfk_dataset.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Dataset.load() to load\n",
    "jfk_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HOURLYVISIBILITY</th>\n",
       "      <th>HOURLYDRYBULBTEMPF</th>\n",
       "      <th>HOURLYWETBULBTEMPF</th>\n",
       "      <th>HOURLYDewPointTempF</th>\n",
       "      <th>HOURLYRelativeHumidity</th>\n",
       "      <th>HOURLYWindSpeed</th>\n",
       "      <th>HOURLYStationPressure</th>\n",
       "      <th>HOURLYSeaLevelPressure</th>\n",
       "      <th>HOURLYPrecip</th>\n",
       "      <th>HOURLYAltimeterSetting</th>\n",
       "      <th>HOURLYWindDirectionSin</th>\n",
       "      <th>HOURLYWindDirectionCos</th>\n",
       "      <th>HOURLYPressureTendencyIncr</th>\n",
       "      <th>HOURLYPressureTendencyDecr</th>\n",
       "      <th>HOURLYPressureTendencyCons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01 01:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.97</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-01 02:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.97</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-01 03:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.97</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-01 04:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.95</td>\n",
       "      <td>29.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-01 05:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.93</td>\n",
       "      <td>29.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DATE  HOURLYVISIBILITY  HOURLYDRYBULBTEMPF  \\\n",
       "0 2010-01-01 01:00:00               6.0                33.0   \n",
       "1 2010-01-01 02:00:00               6.0                33.0   \n",
       "2 2010-01-01 03:00:00               5.0                33.0   \n",
       "3 2010-01-01 04:00:00               5.0                33.0   \n",
       "4 2010-01-01 05:00:00               5.0                33.0   \n",
       "\n",
       "   HOURLYWETBULBTEMPF  HOURLYDewPointTempF  HOURLYRelativeHumidity  \\\n",
       "0                32.0                 31.0                    92.0   \n",
       "1                33.0                 32.0                    96.0   \n",
       "2                33.0                 32.0                    96.0   \n",
       "3                33.0                 32.0                    96.0   \n",
       "4                32.0                 31.0                    92.0   \n",
       "\n",
       "   HOURLYWindSpeed  HOURLYStationPressure  HOURLYSeaLevelPressure  \\\n",
       "0              0.0                  29.97                   29.99   \n",
       "1              0.0                  29.97                   29.99   \n",
       "2              0.0                  29.97                   29.99   \n",
       "3              0.0                  29.95                   29.97   \n",
       "4              0.0                  29.93                   29.96   \n",
       "\n",
       "   HOURLYPrecip  HOURLYAltimeterSetting  HOURLYWindDirectionSin  \\\n",
       "0          0.01                   29.99                     0.0   \n",
       "1          0.02                   29.99                     0.0   \n",
       "2          0.00                   29.99                     0.0   \n",
       "3          0.00                   29.97                     0.0   \n",
       "4          0.00                   29.95                     0.0   \n",
       "\n",
       "   HOURLYWindDirectionCos  HOURLYPressureTendencyIncr  \\\n",
       "0                     1.0                           0   \n",
       "1                     1.0                           0   \n",
       "2                     1.0                           0   \n",
       "3                     1.0                           0   \n",
       "4                     1.0                           0   \n",
       "\n",
       "   HOURLYPressureTendencyDecr  HOURLYPressureTendencyCons  \n",
       "0                           1                           0  \n",
       "1                           1                           0  \n",
       "2                           1                           0  \n",
       "3                           1                           0  \n",
       "4                           1                           0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOAA JFK Weather is a CSV dataset and by default CSV datasets are loaded into Pandas dataframes\n",
    "jfk_dataset.data['jfk_weather_cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/.pydax.dataset/files.list\r\n",
      "8.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/.pydax.dataset\r\n",
      "29M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/jfk_weather.csv\r\n",
      "12K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/LICENSE.txt\r\n",
      "5.8M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/jfk_weather_cleaned.csv\r\n",
      "8.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/clean_data.py\r\n",
      "4.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/README.txt\r\n",
      "35M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport\r\n",
      "35M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4\r\n",
      "35M\t/home/wsuser/new-pydax-datadir/jfk\r\n",
      "35M\t/home/wsuser/new-pydax-datadir\r\n"
     ]
    }
   ],
   "source": [
    "# Show datadir structure\n",
    "!du -ah ~/new-pydax-datadir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Custom User Schema\n",
    "\n",
    "PyDAX supports users defining their own custom schemata and using those to download and load their datasets. Lets as an example use a custom schema for the IBM Debater Concept Abstractness dataset which will be downloaded from Box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ibm.box.com/shared/static/uzw72y44ghxujgcyit6kmxhg3m8va8pj.yaml'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify custom datasets schema which contains the IBM Debater Concept Abstractness dataset\n",
    "custom_datasets_schema_path = 'https://ibm.box.com/shared/static/uzw72y44ghxujgcyit6kmxhg3m8va8pj.yaml'\n",
    "pydax.init(DATASET_SCHEMA_URL=custom_datasets_schema_path)\n",
    "pydax.get_config().DATASET_SCHEMA_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'IBM Debater Concept Abstractness',\n",
       " 'published': datetime.date(2019, 6, 29),\n",
       " 'homepage': 'https://developer.ibm.com/exchanges/data/all/concept-abstractness/',\n",
       " 'download_url': 'https://dax-cdn.cdn.appdomain.cloud/dax-concept-abstractness/1.0.2/concept-abstractness.tar.gz',\n",
       " 'sha512sum': '25cb76c0a8fdfc9cae7e050d4c2492bf055f97a20fa85690b9aaf7dcf965a705fc89e32aed7fbb6d418432e5368cb06fc3bb0f1ab85807fec8aef9df3965cc06',\n",
       " 'license': 'cdla_sharing',\n",
       " 'estimated_size': '3.6M',\n",
       " 'description': 'Abstractness quantifies the degree to which an expression denotes an entity that can be directly perceived by human senses.',\n",
       " 'subdatasets': {'prediction_unigrams': {'name': 'Prediction Unigrams',\n",
       "   'description': 'Concepts and abstractness scores for unigrams (single worded concepts)',\n",
       "   'format': {'id': 'csv', 'options': {'encoding': 'UTF-8'}},\n",
       "   'path': 'prediction_unigrams.csv'},\n",
       "  'prediction_bigrams': {'name': 'Prediction Bigrams',\n",
       "   'description': 'Concepts and abstractness scores for bigrams (two word concepts)',\n",
       "   'format': {'id': 'csv', 'options': {'encoding': 'UTF-8'}},\n",
       "   'path': 'prediction_bigrams.csv'},\n",
       "  'prediction_trigrams': {'name': 'Prediction Trigrams',\n",
       "   'description': 'Concepts and abstractness scores for trigrams (three word concepts)',\n",
       "   'format': {'id': 'csv', 'options': {'encoding': 'UTF-8'}},\n",
       "   'path': 'prediction_trigrams.csv'}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load custom dataset schema for IBM Debater Concept Abstractness dataset\n",
    "custom_schema_manager = pydax.export_schemata()\n",
    "concept_abstractness_schema = custom_schema_manager.schemata['datasets'].export_schema('datasets', 'concept_abstractness', '1.0.2')\n",
    "concept_abstractness_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load Concept Abstractness dataset using the Dataset class\n",
    "concept_abstractness_data_dir = pydax.get_config().DATADIR / 'concept-abstractness' / '1.0.2'\n",
    "concept_abstractness_dataset = Dataset(schema=concept_abstractness_schema,\n",
    "                                       data_dir=concept_abstractness_data_dir,\n",
    "                                       mode=Dataset.InitializationMode.DOWNLOAD_AND_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prediction_unigrams', 'prediction_bigrams', 'prediction_trigrams'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at subdatasets that were loaded, Concept Abstractness has 3 separate subdatasets\n",
    "concept_abstractness_dataset.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a best</td>\n",
       "      <td>0.088837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bola</td>\n",
       "      <td>0.254723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a famosa</td>\n",
       "      <td>0.135748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a fazenda</td>\n",
       "      <td>0.187551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a gallery</td>\n",
       "      <td>0.486240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Concept     Score\n",
       "0     a best  0.088837\n",
       "1     a bola  0.254723\n",
       "2   a famosa  0.135748\n",
       "3  a fazenda  0.187551\n",
       "4  a gallery  0.486240"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since Concept Abstractness is also a CSV dataset, it also gets loaded into a Pandas dataframe\n",
    "# Note: The user can however define their own custom data loader if they prefer to load the dataset in a different way\n",
    "concept_abstractness_dataset.data['prediction_bigrams'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/.pydax.dataset/files.list\r\n",
      "8.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/.pydax.dataset\r\n",
      "29M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/jfk_weather.csv\r\n",
      "12K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/LICENSE.txt\r\n",
      "5.8M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/jfk_weather_cleaned.csv\r\n",
      "8.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/clean_data.py\r\n",
      "4.0K\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport/README.txt\r\n",
      "35M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4/noaa-weather-data-jfk-airport\r\n",
      "35M\t/home/wsuser/new-pydax-datadir/jfk/1.1.4\r\n",
      "35M\t/home/wsuser/new-pydax-datadir/jfk\r\n",
      "2.7M\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2/prediction_bigrams.csv\r\n",
      "24K\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2/LICENSE.txt\r\n",
      "3.3M\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2/prediction_trigrams.csv\r\n",
      "4.0K\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2/README.txt\r\n",
      "4.0K\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2/.pydax.dataset/files.list\r\n",
      "8.0K\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2/.pydax.dataset\r\n",
      "2.2M\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2/prediction_unigrams.csv\r\n",
      "8.1M\t/home/wsuser/new-pydax-datadir/concept-abstractness/1.0.2\r\n",
      "8.1M\t/home/wsuser/new-pydax-datadir/concept-abstractness\r\n",
      "43M\t/home/wsuser/new-pydax-datadir\r\n"
     ]
    }
   ],
   "source": [
    "# Show datadir structure\n",
    "!du -ah ~/new-pydax-datadir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Customer User Loader\n",
    "\n",
    "PyDAX uses loaders to load a certain dataset filetype into a certain Python object. For instance we've been using the `CSVPandasLoader`, the default loader used for CSV datasets, to load the NOAA JFK Weather and Concept Abstractness datasets into Pandas dataframes. If PyDAX had not yet implemented a loader a user desires, they may define that loader manually and use it when loading their own datasets. All PyDAX loaders inherit from a base class called `Loader` which expects you to overwrite the `Loader.load` method. This method is run each time a subdataset is loaded during a `Dataset.load` call.\n",
    "\n",
    "Lets for demonstration purposes define a simple custom loader that loads CSV files into strings instead of Pandas dataframes. To do this, we first define a class called `CSVStringLoader` which inherits from `Loader`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVStringLoader(Loader):\n",
    "    def load(self, path, options):\n",
    "        \"\"\"Custom loader to load CSV files into strings (for demo purposes).\n",
    "\n",
    "        :param path: The path to the subdataset CSV file.\n",
    "        :param options:\n",
    "               - ``encoding`` key specifies the encoding of the CSV file.\n",
    "        \"\"\"\n",
    "\n",
    "        encoding = options.get('encoding', 'utf-8')\n",
    "        return pathlib.Path(path).read_text(encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our customer loader defined, we must create a new `FormatLoaderMap` instance that registers this loader. If a custom `FormatLoaderMap` instance is not created, PyDAX uses a default `FormatLoaderMap` instance that has access to all of PyDAX's default loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register our custom CSVStringLoader into our custom FormatLoaderMap instance custom_format_loader_map\n",
    "custom_format_loader_map = FormatLoaderMap({'csv': CSVStringLoader()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to load a dataset using our custom loader. Lets reload the Concept Abstractness dataset but this time load its subdatasets into strings instead of Pandas dataframes. To do this, we simply provide our `custom_format_loader_map` instance as an argument to `Dataset.load`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a custom format_loader_map argument\n",
    "concept_abstractness_dataset.load(format_loader_map=custom_format_loader_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept,Score\n",
      "a baby story,0.330729888\n",
      "a bad dream,0.420424387\n",
      "a bailar tour,0.178898939\n",
      "a beautiful lie,0.237211137\n",
      "a benihana christmas,0.330718464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peek at the trigram subdataset which has been loaded in as a string\n",
    "concept_abstractness_bigrams = concept_abstractness_dataset.data['prediction_trigrams']\n",
    "print(concept_abstractness_bigrams[:150])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
